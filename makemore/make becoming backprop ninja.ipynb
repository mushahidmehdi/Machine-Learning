{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "096825a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "72a681ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the vocabulary of the given dataset.\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "allWords = sorted(list(set(''.join(words))))\n",
    "itos = {idx + 1 : alps for idx, alps in enumerate(allWords)}\n",
    "itos[0] = '.'\n",
    "stoi = { idx: alps for alps, idx in itos.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7a3dd052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a dataset \n",
    "\n",
    "block_size = 3  # how many character we want to take in context to predict the next character. \n",
    "\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "    for word in words:\n",
    "        context = [0] * block_size\n",
    "        for alpha in word + '.':\n",
    "            ix = stoi[alpha]\n",
    "            X.append(context)\n",
    "            context = context[1:] + [ix]\n",
    "            Y.append(ix)\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "random.seed(1599)\n",
    "random.shuffle(words)\n",
    "\n",
    "n1 = int(len(words) * 0.8)\n",
    "n2 = int(len(words) * 0.9)\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1]) # 80% of the data\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])  # 10% of the data\n",
    "Xtes, Ytes = build_dataset(words[n2:])  # 10% of the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "692d6224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function.\n",
    "def cmp(s, df, t):\n",
    "    exact = torch.all(df == t.grad).item()\n",
    "    apro = torch.allclose(df, t.grad)\n",
    "    diff = (df - t.grad).abs().max().item()\n",
    "    print(f\"{s:15} ||  exact:{exact} || approximation: {apro} || maxDiff: {diff}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1e266910",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_emb = 10 # the dimensionality of the character embedding.\n",
    "n_hidden = 200 # No of nurons in the hidden layer.\n",
    "vocab_size = len(itos) # vacubulary size: total number of unique character\n",
    "\n",
    "gen = torch.Generator().manual_seed(2147483647) # generator of a seed value for reproducibility.\n",
    "C = torch.randn((vocab_size, n_emb),             generator=gen) # Fra\n",
    "w1 = torch.randn((block_size * n_emb, n_hidden), generator=gen) * (5/3)/((n_emb * block_size) ** 0.5)\n",
    "b1 = torch.randn(n_hidden,                       generator=gen) * 0.1\n",
    "w2 = torch.randn((n_hidden, vocab_size),         generator=gen) * 0.1\n",
    "b2 = torch.randn(vocab_size,                     generator=gen) * 0.1\n",
    "\n",
    "bn_gain = torch.ones((1, n_hidden))\n",
    "bn_bias = torch.zeros((1, n_hidden))\n",
    "\n",
    "parameters = [C,b1, w1, b2, w2, bn_gain, bn_bias]  # all the parameters in the mode.\n",
    "sum(p.nelement() for p in parameters)\n",
    "for par in parameters:\n",
    "    par.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "faeb44eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n = batch_size\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size, ), generator=gen)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1c03e656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.7877, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[Xb]\n",
    "embTrans = emb.view(emb.shape[0], -1)\n",
    "hpredn = embTrans @ w1  + b1\n",
    "\n",
    "# batch normalization.\n",
    "bnmeani = 1/n * hpredn.sum(0, keepdim=True)\n",
    "bndiff = hpredn - bnmeani\n",
    "bndiff2 = bndiff**2\n",
    "bnvar = 1/(n-1) * bndiff2.sum(0, keepdim=True)\n",
    "bnvar_inv = (bnvar + 1e-5) ** -0.5\n",
    "bnraw = bndiff * bnvar_inv\n",
    "hpred = bn_gain * bnraw + bn_bias \n",
    "\n",
    "# Non Linearity.\n",
    "h = torch.tanh(hpred)\n",
    "\n",
    "# passing it through the second layer.\n",
    "logits = h @ w2 + b2\n",
    "\n",
    "# cross entropy loss same as (F.cross_entropy(logits, Yb))\n",
    "logits_max = logits.max(1, keepdim=True).values\n",
    "\n",
    "# subtracts from the max value for numerical stability\n",
    "norm_logits = logits - logits_max\n",
    "counts = norm_logits.exp()\n",
    "count_sum = counts.sum(1, keepdim=True)\n",
    "count_sum_inv = count_sum ** -1\n",
    "probs =  counts * count_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# backward pass\n",
    "# making sure all the grad are None.\n",
    "for param in parameters:\n",
    "    param.grad = None\n",
    "\n",
    "for t in [logprobs, probs, counts, count_sum, count_sum_inv,\n",
    "         norm_logits, logits_max, logits, h, hpred, bnraw, bnvar_inv,\n",
    "         bnvar, bndiff2, bndiff, hpredn, bnmeani, embTrans, emb]:\n",
    "    t.retain_grad()\n",
    "    \n",
    "loss.backward()\n",
    "loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5e00d702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        ||  exact:True || approximation: True || maxDiff: 0.0\n",
      "dprobs          ||  exact:True || approximation: True || maxDiff: 0.0\n",
      "dcount_sum_inv  ||  exact:True || approximation: True || maxDiff: 0.0\n",
      "dcount_sum      ||  exact:True || approximation: True || maxDiff: 0.0\n",
      "dcounts         ||  exact:True || approximation: True || maxDiff: 0.0\n",
      "dnorm_logits    ||  exact:True || approximation: True || maxDiff: 0.0\n",
      "dlogits_max     ||  exact:True || approximation: True || maxDiff: 0.0\n",
      "dlogits         ||  exact:True || approximation: True || maxDiff: 0.0\n",
      "dh              ||  exact:True || approximation: True || maxDiff: 0.0\n",
      "dw2             ||  exact:True || approximation: True || maxDiff: 0.0\n",
      "db2             ||  exact:True || approximation: True || maxDiff: 0.0\n",
      "dhpred          ||  exact:True || approximation: True || maxDiff: 0.0\n",
      "dbn_gain        ||  exact:True || approximation: True || maxDiff: 0.0\n",
      "dbnraw          ||  exact:True || approximation: True || maxDiff: 0.0\n",
      "dbn_bias        ||  exact:True || approximation: True || maxDiff: 0.0\n",
      "dbnvar_inv      ||  exact:True || approximation: True || maxDiff: 0.0\n",
      "dbnvar          ||  exact:True || approximation: True || maxDiff: 0.0\n",
      "dbndiff2        ||  exact:True || approximation: True || maxDiff: 0.0\n",
      "dbndiff         ||  exact:True || approximation: True || maxDiff: 0.0\n",
      "dhpredn         ||  exact:True || approximation: True || maxDiff: 0.0\n",
      "dbnmeani        ||  exact:True || approximation: True || maxDiff: 0.0\n",
      "dembTrans       ||  exact:True || approximation: True || maxDiff: 0.0\n",
      "dw1             ||  exact:True || approximation: True || maxDiff: 0.0\n",
      "dw1             ||  exact:True || approximation: True || maxDiff: 0.0\n",
      "demb            ||  exact:True || approximation: True || maxDiff: 0.0\n",
      "C               ||  exact:True || approximation: True || maxDiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dlogprobs = torch.zeros_like(logprobs)\n",
    "dlogprobs[range(n), Yb] = -1.0/n\n",
    "dprobs = (1/probs) * dlogprobs\n",
    "dcount_sum_inv = (counts * dprobs).sum(1, keepdim=True)\n",
    "dcounts = count_sum_inv * dprobs\n",
    "dcount_sum = (-count_sum**-2) * dcount_sum_inv\n",
    "dcounts += torch.ones_like(counts) * dcount_sum\n",
    "dnorm_logits = counts * dcounts\n",
    "dlogits = dnorm_logits.clone()\n",
    "dlogits_max = (-dnorm_logits).sum(1, keepdim=True)\n",
    "dlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogits_max\n",
    "dh = dlogits @ w2.T\n",
    "dw2 = h.T @ dlogits\n",
    "db2 = dlogits.sum(0)\n",
    "dhpred = (1.0 - h ** 2) * dh\n",
    "dbn_gain =  (bnraw * dhpred).sum(0, keepdim=True)\n",
    "dbnraw = bn_gain * dhpred\n",
    "dbn_bias = dhpred.sum(0, keepdim=True)\n",
    "dbndiff = bnvar_inv * dbnraw\n",
    "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
    "dbnvar =  (-0.5 * (bnvar + 1e-5) ** -1.5) * dbnvar_inv\n",
    "dbndiff2 = (1.0/(n-1)) * torch.ones_like(bndiff2) * dbnvar\n",
    "dbndiff += (2 * bndiff) * dbndiff2\n",
    "dhpredn = dbndiff.clone()\n",
    "dbnmeani = (-dbndiff).sum(0)\n",
    "dhpredn += 1.0/n * (torch.ones_like(dhpredn) * dbnmeani)\n",
    "dembTrans = dhpredn @ w1.T\n",
    "dw1 = embTrans.T @ dhpredn\n",
    "db1 = dhpredn.sum(0)\n",
    "demb = dembTrans.view(emb.shape)\n",
    "\n",
    "dC = torch.zeros_like(C)\n",
    "for k in range(Xb.shape[0]):\n",
    "    for j in range(Xb.shape[1]):\n",
    "        ix = Xb[k,j]\n",
    "        dC[ix] += demb[k, j]\n",
    "                   \n",
    "\n",
    "\n",
    "cmp(\"logprobs\", dlogprobs, logprobs)\n",
    "cmp(\"dprobs\", dprobs, probs)\n",
    "cmp(\"dcount_sum_inv\", dcount_sum_inv, count_sum_inv)\n",
    "cmp(\"dcount_sum\", dcount_sum, count_sum)\n",
    "cmp(\"dcounts\", dcounts, counts)\n",
    "cmp(\"dnorm_logits\", dnorm_logits, norm_logits)\n",
    "cmp(\"dlogits_max\", dlogits_max, logits_max)\n",
    "cmp(\"dlogits\", dlogits, logits)\n",
    "cmp(\"dh\", dh, h)\n",
    "cmp(\"dw2\", dw2, w2)\n",
    "cmp(\"db2\", db2, b2)\n",
    "cmp(\"dhpred\", dhpred, hpred)\n",
    "cmp(\"dbn_gain\", dbn_gain, bn_gain)\n",
    "cmp(\"dbnraw\", dbnraw, bnraw)\n",
    "cmp(\"dbn_bias\", dbn_bias, bn_bias)\n",
    "cmp(\"dbnvar_inv\", dbnvar_inv, bnvar_inv)\n",
    "cmp(\"dbnvar\", dbnvar, bnvar)\n",
    "cmp(\"dbndiff2\", dbndiff2, bndiff2)\n",
    "cmp(\"dbndiff\", dbndiff, bndiff)\n",
    "cmp(\"dhpredn\", dhpredn, hpredn)\n",
    "cmp(\"dbnmeani\", dbnmeani, bnmeani)\n",
    "cmp(\"dembTrans\", dembTrans, embTrans)\n",
    "cmp(\"dw1\", dw1, w1)\n",
    "cmp(\"dw1\", db1, b1)\n",
    "cmp(\"demb\", demb, emb)\n",
    "cmp(\"C\", dC, C)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc5dd6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc3b605",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
