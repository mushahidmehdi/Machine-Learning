{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bec100ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "16030e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sz = 32\n",
    "block_sz = 8\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 1e-3\n",
    "no_embd = 32\n",
    "eval_itters = 200\n",
    "torch.manual_seed(1024)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e8ac83c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('file.txt','r', encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d85dd33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding all the unique char in the `text`\n",
    "unique_chars = ''.join(sorted(set(text)))\n",
    "vocab_sz = len(unique_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "72cc89d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a maping dict from `str` to Ã¬nt` and `int` to `str` for the dataset:\n",
    "stoi = {chr: i for i, chr in enumerate(unique_chars)}\n",
    "itos = {i: chr for chr, i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "202c973d",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = lambda inp : [stoi[alpha] for alpha in inp]\n",
    "decode = lambda inp : [itos[intr] for intr in inp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "689bd74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the encoded data into a tensor.\n",
    "dataset = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "# now let's split the above dataset into train and dev 80% & 20% respect.\n",
    "dataLen = len(dataset)\n",
    "train_per = int(dataLen * 0.8)\n",
    "dev_per = int(dataLen * 0.2)\n",
    "\n",
    "train_data = dataset[:train_per]\n",
    "dev_data = dataset[:dev_per]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "88bed155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a stack batch where the stack size is: 'stack_sz' with batch len of 8.\n",
    "def get_batch(dt):\n",
    "    data = train_data if dt == 'train' else dev_data\n",
    "    ix = torch.randint(len(data) - block_sz, (batch_sz, ))\n",
    "    x = torch.stack([data[i : i + block_sz] for i in ix])\n",
    "    y = torch.stack([data[i + 1 : i + 1 + block_sz] for i in ix])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0801698e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimated_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'eval']:\n",
    "        losses = torch.zeros(eval_itters)\n",
    "        for k in range(eval_itters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            loss[k], = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4ece68db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B, T, C = 4, 8, 32\n",
    "# head_size = 16\n",
    "# x = torch.randn((B, T, C))\n",
    "\n",
    "# key = nn.Linear(C, head_size, bias=False)\n",
    "# query = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "# k = key(x)\n",
    "# q = query(x)\n",
    "\n",
    "# tril = torch.tril(torch.ones(T, T))\n",
    "\n",
    "# wei = q @ k.transpose(-2, -1) * head_size**-0.5\n",
    "\n",
    "# wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "# wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "# value = nn.Linear(C, head_size, bias=False)\n",
    "# v = value(x)\n",
    "\n",
    "# out = wei @ v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "10e654c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" head of self-attention \"\"\"\n",
    "    def __init__(self, head_sz):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(no_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(no_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(no_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_sz, block_sz)))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        wei = q @k.transpose(-2, -1) * C**-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T,:T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "55cfd5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MutiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi Head attention in parallel\"\"\"\n",
    "    \n",
    "    def __init__(self, num_heads, head_sz):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_sz) for _ in range(num_heads)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.cat([h(x) for h in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1eaca618",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageMode(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_sz, no_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_sz, no_embd)\n",
    "        self.sa_heads = MutiHeadAttention(4, no_embd//4)\n",
    "        self.lm_head = nn.Linear(no_embd, vocab_sz)\n",
    "        \n",
    "    \n",
    "    def forward(self, idx, target=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.sa_heads(x)\n",
    "        \n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        if target is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            target = target.view(B*T)\n",
    "            loss = F.cross_entropy(logits, target) \n",
    "            \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_token):\n",
    "        \n",
    "        for _ in range(max_new_token):\n",
    "            idx_cond = logits[:, -block_sz:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_idx = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, next_idx), dim=1)\n",
    "\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "78d2d9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BigramLanguageMode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9d90f016",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (256x64 and 32x65)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[91], line 11\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iters):\n\u001b[1;32m      5\u001b[0m     \n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#     if iter % eval_interval == 0:\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#         losses = estimated_loss()\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#         print(f\"step {iter}: train loss {losses['train']:.4f }, val loss {losses['val']:.4f }\")\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     xb, yb \u001b[38;5;241m=\u001b[39m get_batch(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m     logits, loss \u001b[38;5;241m=\u001b[39m model(xb, yb)\n\u001b[1;32m     12\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     14\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[85], line 18\u001b[0m, in \u001b[0;36mBigramLanguageMode.forward\u001b[0;34m(self, idx, target)\u001b[0m\n\u001b[1;32m     15\u001b[0m x \u001b[38;5;241m=\u001b[39m tok_emb \u001b[38;5;241m+\u001b[39m pos_emb\n\u001b[1;32m     16\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msa_heads(x)\n\u001b[0;32m---> 18\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(x)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     21\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (256x64 and 32x65)"
     ]
    }
   ],
   "source": [
    "# traning the model.\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-2)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    \n",
    "#     if iter % eval_interval == 0:\n",
    "#         losses = estimated_loss()\n",
    "#         print(f\"step {iter}: train loss {losses['train']:.4f }, val loss {losses['val']:.4f }\")\n",
    "    \n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "context = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(decode(model.generate(context, max_new_token=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb5d4b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
